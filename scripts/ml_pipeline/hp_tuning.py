from collections import Counter
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
import pandas as pd
import numpy as np

categorical_columns = [
    'impact.baseMetricV2.cvssV2.accessVector', 
    'impact.baseMetricV2.cvssV2.authentication', 
    'impact.baseMetricV2.cvssV2.accessComplexity',
    'impact.baseMetricV2.cvssV2.integrityImpact',
    'impact.baseMetricV2.cvssV2.confidentialityImpact',
    'impact.baseMetricV2.cvssV2.availabilityImpact',
    'impact.baseMetricV2.severity',
    'impact.baseMetricV2.obtainAllPrivilege',
    'impact.baseMetricV2.obtainUserPrivilege',
    'impact.baseMetricV2.obtainOtherPrivilege',
    'impact.baseMetricV2.userInteractionRequired'
]

def data_preparation(df):

    # Dropping columns that are not needed
    df = df.drop(labels="Unnamed: 0", axis=1)
    df = df.drop(labels="impact.baseMetricV2.acInsufInfo", axis=1)    # Clearing the dataset from NA values
    df = df[df['impact.baseMetricV2.userInteractionRequired'].notna()]
    encoder = OrdinalEncoder()
    df[categorical_columns] = encoder.fit_transform(df[categorical_columns])
    X = df.iloc[:,22:36].values  
    y = df.iloc[ :, 5].values 
    return X, y

if __name__ == "__main__":
    # Dataset import and data preparation phase
    df = pd.read_csv('../../data/final/dataset.csv')  
    X, y = data_preparation(df)
    # Splitting the dataset in training and test set
    test_set_size = 0.2
    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= test_set_size, random_state=0) 
    X_train, y_train = X_train, y_train
    print(f"Training set before undersampling: {Counter(y_train)}")
    #print(f"Training set after undersampling: {Counter(y_train)}")
    print(f"Test set: {Counter(y_test)}")

    rf_classifier = RandomForestClassifier()

    # Random grid for Random Forest hyperparameters tuning
    n_estimators = np.arange(100, 500, step=100)
    max_features = ["sqrt", "log2"]
    max_depth = list(np.arange(10, 50, step=10)) + [None]
    min_samples_split = np.arange(2, 10, step=2)
    min_samples_leaf = [1, 2]
    bootstrap = [True, False]

    # Create the random grid
    rf_param_grid = {'n_estimators': n_estimators,
                'max_features': max_features,
                'max_depth': max_depth,
                'min_samples_split': min_samples_split,
                'min_samples_leaf': min_samples_leaf,
                'bootstrap': bootstrap}
    rf_grid_search = GridSearchCV(rf_classifier,
                            rf_param_grid, scoring='f1', n_jobs=-1, cv=5, verbose=3)
    rf_grid_search.fit(X_train, y_train)
    print(f'Best configuration for Random Forest: {rf_grid_search.best_estimator_}')
    # Save best configuration for Random Forest to a text file
    with open('best_configurations.txt', 'w') as file:
        file.write(f'Best configuration for Random Forest: {rf_grid_search.best_estimator_}\n')


    svm_classifier = svm.SVC()
    svm_param_grid = {'C': [0.1, 1, 5, 10], 
                'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
                'kernel': ['rbf', 'linear', ]
                } 
    svm_grid_search = GridSearchCV(svm_classifier,
                            svm_param_grid, scoring='f1', n_jobs=-1, refit= True, cv=5, verbose = 3)
    svm_grid_search.fit(X_train, y_train)
    print(f'Best configuration for SVM: {svm_grid_search.best_estimator_}')
    # Append best configuration for SVM to the text file
    with open('best_configurations.txt', 'a') as file:
        file.write(f'Best configuration for SVM: {svm_grid_search.best_estimator_}\n')

    # Decision tree
    tree_classifier = DecisionTreeClassifier(random_state=42)
    dt_param_grid = {'criterion': ['gini', 'entropy'],
                'splitter': ['best', 'random'],
                'max_depth': [3, 4, 5, 6, 10],
                'min_samples_split': [2, 3, 4],
                'min_samples_leaf': [1, 2, 3],
                'max_features': [None, 'sqrt', 'log2'],
                'max_leaf_nodes': [None, 10, 20, 30],
                'min_impurity_decrease': [0.0, 0.1, 0.2]}
    dt_grid_search = GridSearchCV(tree_classifier,
                            dt_param_grid, scoring='f1', n_jobs=-1, refit= True, cv=5, verbose = 3)
    dt_grid_search.fit(X_train, y_train)
    print(f'Best configuration for DecisionTree: {dt_grid_search.best_estimator_}')

    # Append best configuration for DecisionTree to the text file
    with open('best_configurations.txt', 'a') as file:
        file.write(f'Best configuration for DecisionTree: {dt_grid_search.best_estimator_}\n')

    # Naive Bayes
    nb_classifier = BernoulliNB()
    nb_param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0],
            'fit_prior': [True, False],
            }
    nb_grid_search = GridSearchCV(nb_classifier, nb_param_grid, scoring='f1', n_jobs=-1, cv=5, verbose=3)
    nb_grid_search.fit(X_train, y_train)
    print(f'Best configuration for Naive Bayes: {nb_grid_search.best_estimator_}')
    
    # Append best configuration for DecisionTree to the text file
    with open('best_configurations.txt', 'a') as file:
        file.write(f'Best configuration for DecisionTree: {nb_grid_search.best_estimator_}\n')
