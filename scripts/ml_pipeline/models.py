import csv
import gc
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours, ClusterCentroids, AllKNN
from imblearn.over_sampling import SMOTE, RandomOverSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import matthews_corrcoef, _scorer
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
import numpy as np

# Hyperparameter grids for the single models
def param_grids(model):

    if isinstance(model, DecisionTreeClassifier):
        param_grid = {
            'criterion': ['gini', 'entropy'],
            'splitter': ['best', 'random'],
            'max_depth': [3, 4, 5, 6, 10],
            'min_samples_split': [2, 3, 4],
            'min_samples_leaf': [1, 2, 3],
            'max_features': [None, 'sqrt', 'log2'],
            'max_leaf_nodes': [None, 10, 20, 30],
            'min_impurity_decrease': [0.0, 0.1, 0.2]
        }

    if isinstance(model, RandomForestClassifier):
        param_grid = {
            'n_estimators': np.arange(100, 500, step=100),
            'max_features': ["sqrt", "log2"],
            'max_depth': list(np.arange(10, 50, step=10)) + [None],
            'min_samples_split':  np.arange(2, 10, step=2),
            'min_samples_leaf': [1, 2],
            'bootstrap': [True, False]
        }

    if isinstance(model, SVC):
        param_grid = {
            'C': [0.1, 1, 5, 10], 
            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
            'kernel': ['rbf', 'linear']
        } 
        
    if isinstance(model, BernoulliNB):
        param_grid = {
            'alpha': [0.01, 0.1, 0.5, 1.0],
            'fit_prior': [True, False],
        }
    return param_grid

# Running data processing and GridSearchCV (with 5 folds) inside an outer loop of 10-fold cross validation for more accurate results
def hp_tuning(clf, X, y):
    with open('logs/hp_tuning.csv', 'a') as f:
        try:
            writer = csv.writer(f)
            writer.writerow(["Classifier", "Best Configuration", "Best F1-Score", "Outer Cross Validation Scores"])
            print(f"Training set: {Counter(y)}")
            n_folds = 10
            outer_kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
            outer_scores = []
            for train_index, test_index in outer_kf.split(X):
                under_sampler = NearMiss(version=2)
                print(f"Undersampling training data using {under_sampler.__class__.__name__} algorithm")
                X_train_outer, X_test_outer = X[train_index], X[test_index]
                y_train_outer, y_test_outer = y[train_index], y[test_index]
                print(f"Training set before undersampling: {Counter(y_train_outer)}")
                X_resampled, y_resampled = under_sampler.fit_resample(X_train_outer, y_train_outer)
                print(f"Training set after undersampling: {Counter(y_resampled)}")
                inner_kf = KFold(n_splits=5, shuffle=True, random_state=42)
                grid_search = GridSearchCV(clf, param_grids(clf), scoring='f1', cv=inner_kf, verbose=1)
                grid_search.fit(X_resampled, y_resampled)
                best_param = grid_search.best_params_
                best_score = grid_search.best_score_
                best_clf = grid_search.best_estimator_
                outer_score = cross_val_score(best_clf, X_resampled, y_resampled, scoring='f1', cv=inner_kf)
                writer.writerow([clf.__class__.__name__, best_param, best_score, outer_score])
                outer_scores.append(outer_score.mean())
                del X_train_outer, X_test_outer, y_train_outer, best_param, best_clf, best_score
                gc.collect()
                
            print("Outer CV Scores:", outer_scores)
            print("Mean Outer CV Score:", np.mean(outer_scores))
        except KeyboardInterrupt:
            print("KeyboardInterrupt detected. Closing the Hyperparameters Tuning report file gracefully.")
            f.close()
            exit(0)
        finally:
            f.close()

def samplers_evaluation(clf, X, y):
    print(f"Running algorithms on classifier {clf.__class__.__name__}")
    print(f"Set size before sampling algorithm (based on the y array): {Counter(y)}")
    samplers = [
        RandomUnderSampler(sampling_strategy="majority", random_state=42),
        NearMiss(version=1),
        TomekLinks(),
        NearMiss(version=2),
        EditedNearestNeighbours(),
        NearMiss(version=3),
        ClusterCentroids(random_state=42),
        AllKNN(),
        SMOTE(),
        RandomOverSampler(sampling_strategy="minority", random_state=42)
    ]

    scorers = [
        'accuracy',
        'recall',
        'precision',
        'roc_auc',
        'f1',
        _scorer.make_scorer(matthews_corrcoef)
    ]
    with open('logs/samplers_eval.csv', 'a') as f:
        try:
            # Building a CSV file for results evaluation with a "Classifier, Sampler, Metric, Cross Validation scores" structure
            writer = csv.writer(f)
            writer.writerow(["Classifier", "Sampling Algorithm", "Metric", "Scores"])
            for sampler in samplers:
                X_res, y_res = sampler.fit_resample(X, y)
                print(f"Currently evaluating: {sampler.__class__.__name__}")
                print(f"Set size after sampling algorithm (based on the y array): {Counter(y_res)}")
                for s in scorers:
                    cv_result = cross_val_score(clf, X_res, y_res, cv=5, scoring=s)
                    print(f"Cross validation (on 5 folds) {s} scores: {cv_result}\n")
                    writer.writerow([clf.__class__.__name__, sampler.__class__.__name__, s, cv_result])
                del X_res, y_res, cv_result
                gc.collect()
            f.close()
        except KeyboardInterrupt:
            print("KeyboardInterrupt detected. Closing the Samplers Evaluation report file gracefully.")
            f.close()
            exit(0)
        finally:
            f.close()

def get_baseline_results(clf, X, y):
    scorers = [
        'accuracy',
        'recall',
        'precision',
        'roc_auc',
        'f1',
        _scorer.make_scorer(matthews_corrcoef)
    ]

    with open('logs/baseline_eval.csv', 'a') as f:
        try:
            # Building a CSV file for results evaluation with a "Classifier, Metric, Cross Validation scores" structure
            writer = csv.writer(f)
            writer.writerow(["Classifier", "Metric", "Scores"])
            for s in scorers:
                cv_result = cross_val_score(clf, X, y, cv=5, scoring=s)
                print(f"Cross validation (on 5 folds) {s} scores: {cv_result}\n")
                writer.writerow([clf.__class__.__name__,  s, cv_result])
            del cv_result
            gc.collect()
            f.close()
        except KeyboardInterrupt:
            print("KeyboardInterrupt detected. Closing the baseline scorer report file gracefully.")
            f.close()
            exit(0)
        finally:
            f.close()