import argparse
import gc
from collections import Counter
from imblearn.under_sampling import NearMiss
from sklearn.preprocessing import OrdinalEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.dummy import DummyClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, fbeta_score, roc_auc_score, matthews_corrcoef
from sklearn.model_selection import train_test_split, KFold
from tabulate import tabulate 
import pandas as pd
import numpy as np
import models

categorical_columns = [
    'impact.baseMetricV2.cvssV2.accessVector', 
    'impact.baseMetricV2.cvssV2.authentication', 
    'impact.baseMetricV2.cvssV2.accessComplexity',
    'impact.baseMetricV2.cvssV2.integrityImpact',
    'impact.baseMetricV2.cvssV2.confidentialityImpact',
    'impact.baseMetricV2.cvssV2.availabilityImpact',
    'impact.baseMetricV2.severity',
    'impact.baseMetricV2.obtainAllPrivilege',
    'impact.baseMetricV2.obtainUserPrivilege',
    'impact.baseMetricV2.obtainOtherPrivilege',
    'impact.baseMetricV2.userInteractionRequired'
]

# Drops columns that are not needed, cleans data from NA values and divides the dataset in features and target variable.
def data_preparation(df):
    df = df.drop(labels="Unnamed: 0", axis=1)
    df = df.drop(labels="impact.baseMetricV2.acInsufInfo", axis=1)
    df = df[df['impact.baseMetricV2.userInteractionRequired'].notna()]
    encoder = OrdinalEncoder()
    df[categorical_columns] = encoder.fit_transform(df[categorical_columns])
    X = df.iloc[:,22:36].values  
    y = df.iloc[ :, 5].values 
    return X, y

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = "")
    parser.add_argument("-t", "--tuning", action="store_true", help = "Run hyperparameters tuning")
    parser.add_argument("-s", "--sampling", action="store_true", help = "Run samplers evaluation")
    args = parser.parse_args()
    df = pd.read_csv('../../data/final/dataset.csv')  
    X, y = data_preparation(df)
    del df
    gc.collect()
    test_set_size = 0.2
    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= test_set_size, random_state=42)
    print(f"Divided dataset in training set ({100 - test_set_size * 100}%) and test set ({test_set_size * 100}%)")

    classifiers = [RandomForestClassifier(random_state=42, n_jobs=-1), SVC(), DecisionTreeClassifier(random_state=42), BernoulliNB(), DummyClassifier(strategy="constant", constant = 0), DummyClassifier(strategy="constant", constant = 1)]

    for clf in classifiers:
        if args.tuning: 
            print("Running hyperparameter tuning...")
            models.hp_tuning(clf, X_train, y_train)
        if args.sampling: 
            print("Running samplers evaluation...")
            models.samplers_evaluation(clf, X_train, y_train)