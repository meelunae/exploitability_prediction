from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.pipeline import Pipeline
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, fbeta_score
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
import pickle
from tabulate import tabulate 
import pandas as pd
import numpy as np

def data_preparation(df):
    categorical_columns = [
        'impact.baseMetricV2.cvssV2.accessVector', 
        'impact.baseMetricV2.cvssV2.authentication', 
        'impact.baseMetricV2.cvssV2.accessComplexity',
        'impact.baseMetricV2.cvssV2.integrityImpact',
        'impact.baseMetricV2.cvssV2.confidentialityImpact',
        'impact.baseMetricV2.cvssV2.availabilityImpact',
        'impact.baseMetricV2.severity',
        'impact.baseMetricV2.obtainAllPrivilege',
        'impact.baseMetricV2.obtainUserPrivilege',
        'impact.baseMetricV2.obtainOtherPrivilege',
        'impact.baseMetricV2.userInteractionRequired'
    ]
#     Dropping columns that are not needed
    df = df.drop(labels="Unnamed: 0", axis=1)
    df = df.drop(labels="impact.baseMetricV2.acInsufInfo", axis=1)
#     Clearing the dataset from NA values
    df = df[df['impact.baseMetricV2.userInteractionRequired'].notna()]
    encoder = LabelEncoder()
    for col in categorical_columns:
        df[col] = encoder.fit_transform(df[col])
    X = df.iloc[:,22:36].values  
    y = df.iloc[ :, 5].values 
    return X, y

def cross_validate(classifier, X_train, y_train):
    X_train = pd.DataFrame(X_train)
    y_train = pd.Series(y_train)
    n_folds = 10
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_index, test_index) in enumerate(kf.split(X_train)):
        X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]
        X_test_fold, y_test_fold = X_train.iloc[test_index], y_train.iloc[test_index]
        classifier.fit(X_train_fold, y_train_fold)
        y_test_pred = classifier.predict(X_test_fold)
        fold_score = accuracy_score(y_test_fold, y_test_pred)
        cv_scores.append(fold_score)
        
    mean_cv_score = np.mean(cv_scores)
    print('Cross-validation accuracy for classifier {}: {:.3f})'.format(str(type(classifier)).split(".")[-1][:-2],mean_cv_score))

def my_scorer(classifier, X, y):    
    y_pred = classifier.predict(X)
    cm = confusion_matrix(y, y_pred)
    tn = cm[0, 0]
    fp = cm[0, 1]
    fn = cm[1, 0]
    tp = cm[1, 1]
    inspection_rate = (tp + fp) / (tp + tn + fp + fn)
    precision = precision_score(y, y_pred)
    recall = recall_score(y, y_pred)
    accuracy = accuracy_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    f2 = fbeta_score(y, y_pred, beta=2) 

    headers = ["Metric", "Value"]
    rows = [
        ["Classifier", str(type(classifier)).split(".")[-1][:-2]],
        ["Precision", precision],
        ["Recall", recall],
        ["Accuracy", accuracy],
        ["Inspection Rate", inspection_rate],
        ["F1-score", f1],
        ["F2-score", f2]
    ]    
    print("{}\n".format(tabulate(rows, headers=headers)))

if __name__ == "__main__":
    # Dataset import and data preparation phase
    df = pd.read_csv('../../data/final/dataset.csv')  
    X, y = data_preparation(df)
    # Splitting the dataset in training and test set
    test_set_size = 0.2
    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= test_set_size, random_state=0) 
    # Undersampling since the problem is heavily unbalanced
    under_sampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)
    X_resampled, y_resampled = under_sampler.fit_resample(X_train, y_train)
    print(f"Training set before undersampling: {Counter(y_train)}")
    print(f"Training set after undersampling: {Counter(y_resampled)}")
    print(f"Test set: {Counter(y_test)}")

    # These are the four classifiers that we currently run on the dataset

    rf_classifier = RandomForestClassifier(max_depth=16, max_features='log2', min_samples_leaf=8,
                        min_samples_split=10, n_estimators=20)

    # Random grid for Random Forest hyperparameters tuning
    n_estimators = [int(x) for x in np.arange(20,200,15)]
    max_features = ['log2', 'sqrt']
    max_depth = [int(x) for x in np.linspace(4, 16, num = 6)]
    max_depth.append(None)
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [2, 4, 8]
    bootstrap = [True, False]
    # Create the random grid
    param_grid = {'n_estimators': n_estimators,
                'max_features': max_features,
                'max_depth': max_depth,
                'min_samples_split': min_samples_split,
                'min_samples_leaf': min_samples_leaf,
                'bootstrap': bootstrap}
    grid_search = GridSearchCV(rf_classifier,
                            param_grid, verbose=3)
    #grid_search.fit(X_resampled, y_resampled)
    #print(f'Best configuration for Random Forest: {grid_search.best_estimator_}')

    rf_classifier.fit(X_resampled, y_resampled)
    cross_validate(rf_classifier, X_resampled, y_resampled)
    my_scorer(rf_classifier, X_test, y_test)

    svm_classifier = svm.SVC(C=10, gamma=1, kernel="rbf")
    svm_classifier.fit(X_resampled, y_resampled)
    cross_validate(svm_classifier, X_resampled, y_resampled)
    param_grid = {'C': [0.1, 1, 10, 100, 1000], 
                'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
                'kernel': ['rbf', 'linear', ]} 
    grid_search = GridSearchCV(svm_classifier,
                            param_grid, refit= True, verbose = 3)
    #grid_search.fit(X_resampled, y_resampled)
    # print best parameter after tuning
    #print(grid_search.best_params_)
    
    # print how our model looks after hyper-parameter tuning
    #print(grid_search.best_estimator_)
    my_scorer(svm_classifier, X_test, y_test)

    print(f'{tabulate(my_scorer(svm_classifier, X_test, y_test), headers="keys")} \n')

    # Decision tree
    tree_classifier = DecisionTreeClassifier(random_state=42)
    tree_classifier.fit(X_resampled, y_resampled)
    cross_validate(tree_classifier, X_resampled, y_resampled)
    my_scorer(tree_classifier, X_test, y_test)
    # Naive Bayes
    nb_classifier = BernoulliNB(alpha=0.01)
    param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0],
            'fit_prior': [True, False],
            }
    grid_search = GridSearchCV(nb_classifier, param_grid, cv=5)
    #grid_search.fit(X_resampled, y_resampled)
    #print(grid_search.best_params_)
    nb_classifier.fit(X_resampled, y_resampled)
    cross_validate(nb_classifier, X_resampled, y_resampled)
    my_scorer(nb_classifier, X_test, y_test)

    # Dumping the trained models to files 
    #pickle.dump(rf_classifier, open('rf.sav', 'wb'))
    #pickle.dump(tree_classifier, open('tree.sav', 'wb'))
    #pickle.dump(nb_classifier, open('nb.sav', 'wb'))
